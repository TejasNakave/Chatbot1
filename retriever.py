import numpy as np
import os
import pickle
import hashlib
import re
from typing import List, Dict, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


def assess_ocr_quality(text: str) -> Dict[str, float]:
    """
    Assess the quality of OCR-extracted text.
    
    Returns:
        Dict with quality scores:
        - corruption_ratio: Ratio of corrupted words (0=clean, 1=fully corrupted)
        - readability_score: Overall readability (0=unreadable, 1=perfect)
        - confidence: Confidence in the assessment (0=low, 1=high)
    """
    if not text or len(text.strip()) < 10:
        return {"corruption_ratio": 0.0, "readability_score": 0.0, "confidence": 0.0}
    
    words = text.split()
    total_words = len(words)
    
    if total_words == 0:
        return {"corruption_ratio": 0.0, "readability_score": 0.0, "confidence": 0.0}
    
    corrupted_words = 0
    
    # Patterns indicating OCR corruption
    corruption_patterns = [
        r'^[a-z]{1,2}$',  # Single/double lowercase letters alone
        r'[0-9][a-z]+[0-9]',  # Numbers mixed with letters randomly
        r'^[A-Z]{3,}$',  # ALL CAPS short words (often OCR artifacts)
        r'[a-zA-Z]{1}[0-9]+[a-zA-Z]{1,2}',  # Letter-number-letter patterns
        r'^[^a-zA-Z0-9\s]{2,}$',  # Multiple special characters
        r'[a-zA-Z]{1,2}[0-9]{1,2}[a-zA-Z]{1,2}',  # Mixed letter-number patterns
    ]
    
    # Common OCR corruption indicators
    corruption_indicators = [
        'entccaate', 'aytion', 'PrctomaBcally', 'alectaredt', 'omplmre',
        'wragiak', 'aperoncn', 'tnenlte', 'Brug', 'docamat', 'poregs'
    ]
    
    for word in words:
        # Check for obvious corruption patterns
        is_corrupted = False
        
        # Pattern-based detection
        for pattern in corruption_patterns:
            if re.match(pattern, word):
                is_corrupted = True
                break
        
        # Known corruption indicators
        if word.lower() in [ind.lower() for ind in corruption_indicators]:
            is_corrupted = True
        
        # Excessive special characters
        if len(re.findall(r'[^a-zA-Z0-9\s]', word)) > len(word) * 0.3:
            is_corrupted = True
        
        # Very short words with mixed case
        if len(word) <= 3 and word.lower() != word and word.upper() != word:
            is_corrupted = True
        
        if is_corrupted:
            corrupted_words += 1
    
    corruption_ratio = corrupted_words / total_words
    readability_score = max(0.0, 1.0 - corruption_ratio * 2)  # Penalize corruption heavily
    confidence = min(1.0, total_words / 50)  # Higher confidence with more text
    
    return {
        "corruption_ratio": corruption_ratio,
        "readability_score": readability_score,
        "confidence": confidence
    }


class SimpleRetriever:
    """
    A document retrieval system using TF-IDF and cosine similarity with caching.
    """
    
    def __init__(self, documents: List[Dict[str, str]], use_cache: bool = True):
        """
        Initialize the retriever with documents.
        
        Args:
            documents (List[Dict[str, str]]): List of document dictionaries
            use_cache (bool): Whether to use caching for TF-IDF vectors
        """
        self.documents = documents
        self.use_cache = use_cache
        self.cache_dir = "cache"
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        self.doc_vectors = None
        self.build_index()
    
    def get_cache_key(self, documents: List[Dict[str, str]]) -> str:
        """Generate a cache key based on document content."""
        content_hash = hashlib.md5()
        for doc in documents:
            content_hash.update(doc.get('content', '').encode('utf-8'))
        return content_hash.hexdigest()[:12]
    
    def load_from_cache(self, cache_key: str) -> Tuple[bool, any, any]:
        """Load cached vectorizer and vectors."""
        try:
            vectorizer_path = os.path.join(self.cache_dir, f"vectorizer_{cache_key}.pkl")
            vectors_path = os.path.join(self.cache_dir, f"vectors_{cache_key}.pkl")
            documents_path = os.path.join(self.cache_dir, f"documents_{cache_key}.pkl")
            
            if all(os.path.exists(p) for p in [vectorizer_path, vectors_path, documents_path]):
                with open(vectorizer_path, 'rb') as f:
                    vectorizer = pickle.load(f)
                with open(vectors_path, 'rb') as f:
                    vectors = pickle.load(f)
                with open(documents_path, 'rb') as f:
                    cached_docs = pickle.load(f)
                
                return True, vectorizer, vectors
        except Exception as e:
            print(f"Cache load error: {e}")
        return False, None, None
    
    def save_to_cache(self, cache_key: str, vectorizer, vectors):
        """Save vectorizer and vectors to cache."""
        try:
            vectorizer_path = os.path.join(self.cache_dir, f"vectorizer_{cache_key}.pkl")
            vectors_path = os.path.join(self.cache_dir, f"vectors_{cache_key}.pkl")
            documents_path = os.path.join(self.cache_dir, f"documents_{cache_key}.pkl")
            
            with open(vectorizer_path, 'wb') as f:
                pickle.dump(vectorizer, f)
            with open(vectors_path, 'wb') as f:
                pickle.dump(vectors, f)
            with open(documents_path, 'wb') as f:
                pickle.dump(self.documents, f)
            
            print(f"ðŸ’¾ Cached TF-IDF data with hash: {cache_key}...")
        except Exception as e:
            print(f"Cache save error: {e}")

    def build_index(self):
        """Build TF-IDF index for documents with caching support."""
        if not self.documents:
            print("No documents to index.")
            return
        
        # Try to load from cache first
        if self.use_cache:
            cache_key = self.get_cache_key(self.documents)
            loaded, vectorizer, vectors = self.load_from_cache(cache_key)
            if loaded:
                self.vectorizer = vectorizer
                self.doc_vectors = vectors
                print(f"ðŸ“¥ Loaded cached TF-IDF data: {cache_key}...")
                print(f"ðŸ“¥ Loaded cached index for {len(self.documents)} documents")
                return
        
        # Build index if not cached
        print(f"ðŸ” Building search index for {len(self.documents)} documents...")
        
        # Extract content from documents
        doc_contents = [doc.get('content', '') for doc in self.documents]
        
        # Create TF-IDF vectors
        self.doc_vectors = self.vectorizer.fit_transform(doc_contents)
        
        # Save to cache
        if self.use_cache:
            cache_key = self.get_cache_key(self.documents)
            self.save_to_cache(cache_key, self.vectorizer, self.doc_vectors)
        
        print(f"Built search index for {len(self.documents)} documents.")
    
    def retrieve_relevant_chunks(self, query: str, top_k: int = 3) -> List[Dict[str, str]]:
        """
        Retrieve the most relevant document chunks for a given query.
        Uses both TF-IDF similarity and exact keyword matching for better results.
        Implements fallback strategies for better recall on question-based queries.
        
        Args:
            query (str): The search query
            top_k (int): Number of top documents to retrieve
            
        Returns:
            List[Dict[str, str]]: Most relevant documents
        """
        if self.doc_vectors is None or not self.documents:
            return []

        try:
            # TOPIC MISMATCH DETECTION - Check if query is about different domain
            query_lower = query.lower()
            
            # Define trade/business terms that our documents are about (excluding generic words)
            trade_terms = ['trade', 'export', 'import', 'customs', 'duty', 'tariff', 'svb', 
                          'business', 'commerce', 'shipping', 'freight', 'documentation', 'fta', 'drawback',
                          'compliance', 'regulation', 'clearance', 'invoice', 'certificate']
            
            # Define terms that indicate completely different topics
            unrelated_topics = ['coffee', 'cooking', 'recipe', 'food', 'kitchen', 'ingredient', 'baking',
                              'personal', 'family', 'health', 'medical', 'sports', 'entertainment', 'music',
                              'movie', 'game', 'hobby', 'travel', 'vacation', 'weather', 'animal', 'pet']
            
            # Check if query contains unrelated terms but no trade terms
            has_unrelated = any(term in query_lower for term in unrelated_topics)
            has_trade_terms = any(term in query_lower for term in trade_terms)
            
            print(f"ðŸ” RETRIEVER CHECK: Query='{query}', has_unrelated={has_unrelated}, has_trade_terms={has_trade_terms}")
            
            # CONTEXTUAL REFERENCE DETECTION - Check if query refers to previous context
            contextual_terms = ['above', 'previous', 'that', 'this', 'those', 'these', 'the image', 'the process', 'earlier']
            has_contextual_reference = any(term in query_lower for term in contextual_terms)
            
            if has_contextual_reference:
                print(f"ðŸ”— CONTEXTUAL QUERY DETECTED: Query references previous context")
                print(f"ðŸ”— Returning empty list to use conversation context instead of search")
                return []  # Let the AI use conversation context instead
            
            # Special handling for coffee queries - allow coffee content even from mixed documents
            if 'coffee' in query_lower:
                print(f"â˜• COFFEE QUERY DETECTED: Looking for coffee-specific content")
                coffee_docs = []
                
                for doc in self.documents:
                    content_lower = doc.get('content', '').lower()
                    file_name = doc.get('file_name', '')
                    
                    # Include documents that contain coffee content
                    if 'coffee' in content_lower:
                        doc_copy = dict(doc)
                        # If it's an SVB document with coffee, mark it as coffee-focused
                        if 'SVB' in file_name.upper():
                            doc_copy['similarity_score'] = 0.90
                            doc_copy['match_type'] = 'coffee_from_svb'
                            print(f"â˜• Found coffee content in SVB document: {doc.get('file_name', '')}")
                        else:
                            doc_copy['similarity_score'] = 0.95  # Higher for pure coffee docs
                            doc_copy['match_type'] = 'coffee_specific'
                            print(f"â˜• Found pure coffee document: {doc.get('file_name', '')}")
                        coffee_docs.append(doc_copy)
                
                if coffee_docs:
                    print(f"â˜• COFFEE RESULTS: Returning {len(coffee_docs)} documents with coffee content")
                    return coffee_docs[:top_k]
                else:
                    print(f"â˜• NO COFFEE CONTENT FOUND: No documents contain coffee information")
                    return []
            
            # Special handling for SVB queries - focus on trade-related SVB content
            if 'svb' in query_lower and not 'coffee' in query_lower:
                print(f"ðŸ›ï¸ SVB QUERY DETECTED: Looking for SVB process content only")
                svb_docs = []
                
                for doc in self.documents:
                    content_lower = doc.get('content', '').lower()
                    file_name = doc.get('file_name', '')
                    
                    # For SVB queries, focus on SVB documents but prefer trade-related content
                    if 'SVB' in file_name.upper():
                        doc_copy = dict(doc)
                        # Check if content has both SVB and coffee - prefer SVB parts
                        if 'svb' in content_lower and 'special valuation' in content_lower:
                            doc_copy['similarity_score'] = 0.95  # High relevance for SVB match
                            doc_copy['match_type'] = 'svb_specific'
                        else:
                            doc_copy['similarity_score'] = 0.85  # Lower for mixed content
                            doc_copy['match_type'] = 'svb_mixed'
                        svb_docs.append(doc_copy)
                        print(f"ðŸ›ï¸ Found SVB document: {doc.get('file_name', '')}")
                
                if svb_docs:
                    print(f"ðŸ›ï¸ SVB RESULTS: Returning {len(svb_docs)} SVB-specific documents")
                    return svb_docs[:top_k]
            
            if has_unrelated and not has_trade_terms:
                print(f"ðŸš« TOPIC MISMATCH: Query about '{query}' is unrelated to trade documents")
                return []  # Return empty list to trigger global search

            # UNIVERSAL SMART DOCUMENT PRIORITIZATION SYSTEM
            # This system handles ALL topics systematically with OCR quality awareness
            query_lower = query.lower()
            forced_results = []
            
            print(f"ðŸ” Query: '{query}' -> Universal smart document selection...")
            
            # Detect query type and intent
            is_definition_query = any(def_word in query_lower for def_word in [
                'what is', 'definition', 'explain', 'meaning', 'define', 'about', 'describe'
            ])
            
            is_flowchart_query = any(flow_word in query_lower for flow_word in [
                'flowchart', 'flow chart', 'process', 'steps', 'procedure', 'show', 'display', 'diagram'
            ])
            
            # TOPIC-SPECIFIC SMART ROUTING
            
            # 1. BILL OF ENTRY (BOE) - Smart routing
            if any(keyword in query_lower for keyword in ['bill of entry', 'boe', 'bill entry', 'entry bill']):
                print("ðŸŽ¯ Detected BILL OF ENTRY (BOE) query")
                if is_definition_query:
                    print("   ðŸ“š Definition query - prioritizing clean DOCX content")
                    # Look for clean text content first
                    for doc in self.documents:
                        content_lower = doc.get('content', '').lower()
                        file_name_lower = doc.get('file_name', '').lower()
                        if ('.docx' in file_name_lower and any(boe_term in content_lower for boe_term in ['bill of entry', 'boe'])):
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'clean_text_boe'
                            forced_results.append(doc_copy)
                            print(f"   âœ… CLEAN CONTENT: {doc.get('file_name', '')} (DOCX)")
                            break
                elif is_flowchart_query:
                    print("   ðŸ”„ Flowchart query - prioritizing visual flowchart")
                    # Look for import process flowchart
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'import process flowchart' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'flowchart_boe'
                            forced_results.append(doc_copy)
                            print(f"   âœ… FLOWCHART: {doc.get('file_name', '')} (Visual)")
                            break
            
            # 2. DUTY DRAWBACK - Smart prioritization
            elif any(keyword in query_lower for keyword in ['duty drawback', 'drawback']):
                print("ðŸŽ¯ Detected DUTY DRAWBACK query")
                if is_definition_query and not is_flowchart_query:
                    print("   ðŸ“š Definition query - prioritizing DOCX content")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'duty drawback content' in file_name_lower and '.docx' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'clean_text_duty'
                            forced_results.append(doc_copy)
                            print(f"   âœ… CLEAN CONTENT: {doc.get('file_name', '')} (DOCX)")
                            break
                elif is_flowchart_query:
                    print("   ðŸ”„ Flowchart query - prioritizing PDF flowchart")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'duty drawback flowchart' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'flowchart_duty'
                            forced_results.append(doc_copy)
                            print(f"   âœ… FLOWCHART: {doc.get('file_name', '')} (Visual)")
                            break
            
            # 3. EXPORT PROCESS - Smart prioritization
            elif any(keyword in query_lower for keyword in ['export process', 'export', 'export procedure']):
                print("ðŸŽ¯ Detected EXPORT PROCESS query")
                if is_definition_query and not is_flowchart_query:
                    print("   ðŸ“š Definition query - prioritizing DOCX content")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'export process contents' in file_name_lower and '.docx' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'clean_text_export'
                            forced_results.append(doc_copy)
                            print(f"   âœ… CLEAN CONTENT: {doc.get('file_name', '')} (DOCX)")
                            break
                elif is_flowchart_query:
                    print("   ðŸ”„ Flowchart query - prioritizing PDF flowchart")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'export process flowchart' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'flowchart_export'
                            forced_results.append(doc_copy)
                            print(f"   âœ… FLOWCHART: {doc.get('file_name', '')} (Visual)")
                            break
            
            # 4. IMPORT PROCESS - Smart prioritization
            elif any(keyword in query_lower for keyword in ['import process', 'import', 'import procedure']):
                print("ðŸŽ¯ Detected IMPORT PROCESS query")
                if is_definition_query and not is_flowchart_query:
                    print("   ðŸ“š Definition query - prioritizing DOCX content")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'import' in file_name_lower and 'process contents' in file_name_lower and '.docx' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'clean_text_import'
                            forced_results.append(doc_copy)
                            print(f"   âœ… CLEAN CONTENT: {doc.get('file_name', '')} (DOCX)")
                            break
                elif is_flowchart_query:
                    print("   ðŸ”„ Flowchart query - prioritizing PDF flowchart")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'import process flowchart' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'flowchart_import'
                            forced_results.append(doc_copy)
                            print(f"   âœ… FLOWCHART: {doc.get('file_name', '')} (Visual)")
                            break
            
            # 5. FREE TRADE AGREEMENT (FTA) - Smart prioritization
            elif any(keyword in query_lower for keyword in ['fta', 'free trade', 'free trade agreement']):
                print("ðŸŽ¯ Detected FREE TRADE AGREEMENT query")
                if is_definition_query and not is_flowchart_query:
                    print("   ðŸ“š Definition query - prioritizing DOCX content")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'free trade agreements content' in file_name_lower and '.docx' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'clean_text_fta'
                            forced_results.append(doc_copy)
                            print(f"   âœ… CLEAN CONTENT: {doc.get('file_name', '')} (DOCX)")
                            break
                elif is_flowchart_query:
                    print("   ðŸ”„ Flowchart query - prioritizing PDF flowchart")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'fta flowchart' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'flowchart_fta'
                            forced_results.append(doc_copy)
                            print(f"   âœ… FLOWCHART: {doc.get('file_name', '')} (Visual)")
                            break
            
            # 6. CUSTOMS TARIFF - Smart prioritization
            elif any(keyword in query_lower for keyword in ['customs tariff', 'tariff', 'hs code', 'customs code']):
                print("ðŸŽ¯ Detected CUSTOMS TARIFF query")
                if is_definition_query and not is_flowchart_query:
                    print("   ðŸ“š Definition query - prioritizing DOCX content")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'customs tariff content' in file_name_lower and '.docx' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'clean_text_tariff'
                            forced_results.append(doc_copy)
                            print(f"   âœ… CLEAN CONTENT: {doc.get('file_name', '')} (DOCX)")
                            break
                elif is_flowchart_query:
                    print("   ðŸ”„ Flowchart query - prioritizing PDF flowchart")
                    for doc in self.documents:
                        file_name_lower = doc.get('file_name', '').lower()
                        if 'customs tarriff & hs code' in file_name_lower and '.pdf' in file_name_lower:
                            doc_copy = dict(doc)
                            doc_copy['similarity_score'] = 0.99
                            doc_copy['match_type'] = 'flowchart_tariff'
                            forced_results.append(doc_copy)
                            print(f"   âœ… FLOWCHART: {doc.get('file_name', '')} (Visual)")
                            break
            
            # 7. GENERAL PROBLEMS/ISSUES QUERIES - Always prioritize comprehensive Q&A document
            elif any(keyword in query_lower for keyword in ['problem', 'issue', 'challenge', 'difficulty', 'trouble']):
                print("ðŸŽ¯ Detected PROBLEMS/ISSUES query")
                for doc in self.documents:
                    file_name_lower = doc.get('file_name', '').lower()
                    if 'problems in import and export' in file_name_lower and '.docx' in file_name_lower:
                        doc_copy = dict(doc)
                        doc_copy['similarity_score'] = 0.99
                        doc_copy['match_type'] = 'comprehensive_problems'
                        forced_results.append(doc_copy)
                        print(f"   âœ… COMPREHENSIVE GUIDE: {doc.get('file_name', '')} (Q&A)")
                        break
            
            # 8. GENERAL FLOWCHART HANDLER - For queries like "show flowchart", "display diagram"
            elif any(keyword in query_lower for keyword in ['show', 'display', 'flowchart', 'flow chart', 'diagram']) and not forced_results:
                print("ðŸŽ¯ Detected GENERAL FLOWCHART/VISUAL query")
                # Look for any flowchart documents and prioritize by topic relevance
                flowchart_docs = []
                for doc in self.documents:
                    file_name_lower = doc.get('file_name', '').lower()
                    if 'flowchart' in file_name_lower:
                        doc_copy = dict(doc)
                        doc_copy['similarity_score'] = 0.90
                        doc_copy['match_type'] = 'general_flowchart'
                        flowchart_docs.append(doc_copy)
                        print(f"   ðŸ“Š AVAILABLE FLOWCHART: {doc.get('file_name', '')}")
                
                if flowchart_docs:
                    # Return the first available flowchart (can be enhanced with better logic later)
                    forced_results.append(flowchart_docs[0])
                    print(f"   âœ… GENERAL FLOWCHART: {flowchart_docs[0].get('file_name', '')}")
            
            # If we found forced results, return them immediately
            if forced_results:
                print(f"ðŸŽ¯ SMART ROUTING SUCCESS: Returning {len(forced_results)} optimally selected documents")
                return forced_results[:top_k]

            # Extract key terms from questions for better search
            def extract_key_terms(query_text):
                """Extract meaningful terms from questions, removing stop words and question words."""
                import re
                # Remove common question words and patterns
                question_words = ['what', 'is', 'are', 'how', 'when', 'where', 'why', 'who', 'which', 'the', 'a', 'an']
                
                # Clean the query
                cleaned = re.sub(r'[^\w\s]', ' ', query_text.lower())
                words = cleaned.split()
                
                # Remove question words and short words
                key_terms = [word for word in words if word not in question_words and len(word) > 2]
                
                return ' '.join(key_terms)

            # Try multiple search strategies
            all_results = []
            
            # Strategy 1: Original query
            results1 = self._search_with_query(query, top_k)
            all_results.extend(results1)
            
            # Strategy 2: If no results from original query, try key terms only
            if not results1:
                key_terms = extract_key_terms(query)
                if key_terms and key_terms != query.lower():
                    results2 = self._search_with_query(key_terms, top_k)
                    all_results.extend(results2)
            
            # Strategy 3: If still no results, try individual significant words
            if not all_results:
                key_terms = extract_key_terms(query)
                words = key_terms.split()
                for word in words:
                    if len(word) > 4:  # Only try longer, more significant words
                        word_results = self._search_with_query(word, top_k)
                        all_results.extend(word_results)
                        if word_results:  # If we find something, stop searching
                            break
            
            # Remove duplicates and sort by similarity
            seen = set()
            unique_results = []
            for doc in all_results:
                doc_id = (doc['file_name'], doc.get('content', '')[:100])  # Use filename + content snippet as ID
                if doc_id not in seen:
                    seen.add(doc_id)
                    unique_results.append(doc)
            
            # Apply OCR quality filtering
            print(f"ðŸ” Applying OCR quality filtering to {len(unique_results)} results...")
            filtered_results = []
            
            for doc in unique_results:
                content = doc.get('content', '')
                quality_assessment = assess_ocr_quality(content)
                
                # Add quality metrics to document
                doc['ocr_quality'] = quality_assessment
                
                # Filter out heavily corrupted content for definition queries
                is_definition_query = any(def_word in query.lower() for def_word in [
                    'what is', 'definition', 'explain', 'meaning', 'define', 'about'
                ])
                
                if is_definition_query and quality_assessment['corruption_ratio'] > 0.4:
                    print(f"   âŒ FILTERED OUT: {doc['file_name']} (corruption: {quality_assessment['corruption_ratio']:.2f})")
                    continue
                elif quality_assessment['corruption_ratio'] > 0.6:  # Very corrupted for any query
                    print(f"   âŒ FILTERED OUT: {doc['file_name']} (severe corruption: {quality_assessment['corruption_ratio']:.2f})")
                    continue
                else:
                    # Adjust similarity score based on quality
                    quality_bonus = quality_assessment['readability_score'] * 0.1
                    doc['similarity_score'] = doc.get('similarity_score', 0) + quality_bonus
                    print(f"   âœ… KEPT: {doc['file_name']} (corruption: {quality_assessment['corruption_ratio']:.2f}, readability: {quality_assessment['readability_score']:.2f})")
                    filtered_results.append(doc)
            
            # Sort by adjusted similarity score and return top results
            filtered_results.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
            print(f"ðŸ” OCR Filtering: {len(unique_results)} â†’ {len(filtered_results)} documents")
            return filtered_results[:top_k]
            
        except Exception as e:
            print(f"Error in retrieval: {e}")
            return []

    def _search_with_query(self, query: str, top_k: int = 3) -> List[Dict[str, str]]:
        """
        Internal method to perform search with a specific query string.
        
        Args:
            query (str): The search query
            top_k (int): Number of top documents to retrieve
            
        Returns:
            List[Dict[str, str]]: Most relevant documents
        """
        try:
            query_lower = query.lower()
            
            # PRIORITY: Prioritize specific flowchart documents for certificate/process queries
            # This helps when comprehensive books compete with specific flowcharts
            if any(term in query_lower for term in ['certificate of origin', 'certificate', 'origin', 'bill of entry', 'boe']):
                print("ðŸ“‹ CERTIFICATE/BOE QUERY: Prioritizing flowchart documents over comprehensive books")
                
                # Find flowchart documents first
                flowchart_docs = []
                comprehensive_docs = []
                
                for doc in self.documents:
                    filename = doc.get('file_name', '').lower()
                    content = doc.get('content', '').lower()
                    
                    # Prioritize specific flowchart files and BOE-containing documents
                    if any(flowchart_term in filename for flowchart_term in [
                        'import process flowchart', 'export process flowchart', 
                        'duty drawback flowchart', 'fta flowchart']) or \
                       any(boe_term in content for boe_term in ['bill of entry', 'boe', '1105807']):
                        flowchart_docs.append(doc)
                        print(f"ðŸ“‹ PRIORITY: {doc.get('file_name', '')}")
                    # Deprioritize comprehensive books
                    elif any(book_term in filename for book_term in [
                        'impex new book', 'comprehensive', 'manual', 'guide']):
                        comprehensive_docs.append(doc)
                        print(f"ðŸ“š DEPRIORITIZED: {doc.get('file_name', '')}")
                    else:
                        flowchart_docs.append(doc)  # Treat other docs as medium priority
                
                # Use flowchart documents first, then add comprehensive if needed
                prioritized_documents = flowchart_docs + comprehensive_docs
                print(f"ðŸ“‹ Using {len(flowchart_docs)} priority docs + {len(comprehensive_docs)} comprehensive docs")
            else:
                prioritized_documents = self.documents
            
            # First, try exact keyword matching for better recall on specific terms
            exact_matches = []
            
            # Extract potential date/keyword patterns from query
            import re
            date_patterns = re.findall(r'\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\s+\d{4}\b', query_lower)
            date_patterns.extend(re.findall(r'\b\d{1,2}[/-]\d{4}\b', query_lower))
            date_patterns.extend(re.findall(r'\b\d{4}[/-]\d{1,2}\b', query_lower))
            
            # Add the full query for exact matching
            search_terms = [query_lower] + date_patterns
            
            for i, doc in enumerate(prioritized_documents):
                content_lower = doc.get('content', '').lower()
                max_score = 0
                match_found = False
                
                for term in search_terms:
                    if term in content_lower:
                        match_found = True
                        # Score based on term length and specificity
                        score = min(1.0, len(term) / 20.0 + 0.5)
                        max_score = max(max_score, score)
                
                if match_found:
                    doc_copy = dict(doc)
                    doc_copy['similarity_score'] = max_score
                    doc_copy['match_type'] = 'exact'
                    exact_matches.append(doc_copy)
            
            # Create TF-IDF vectors for prioritized documents only
            if hasattr(self, '_prioritized_vectors') and len(prioritized_documents) != len(self.documents):
                # Need to rebuild vectors for prioritized subset
                prioritized_contents = [doc.get('content', '') for doc in prioritized_documents]
                prioritized_vectors = self.vectorizer.transform(prioritized_contents)
            else:
                prioritized_vectors = self.doc_vectors
            
            # Vectorize the query for TF-IDF search
            query_vector = self.vectorizer.transform([query])
            
            # Calculate similarity scores
            similarities = cosine_similarity(query_vector, prioritized_vectors).flatten()
            
            # Get top-k most similar documents (limit to available documents)
            num_docs = len(prioritized_documents)
            actual_k = min(top_k, num_docs)
            top_indices = np.argsort(similarities)[::-1][:actual_k]
            
            # Get TF-IDF matches
            tfidf_matches = []
            for i in range(len(top_indices)):
                idx = int(top_indices[i])  # Convert to Python int
                similarity_score = float(similarities[idx])  # Convert to Python float
                
                # Apply minimum similarity threshold (lowered for better recall)
                if similarity_score > 0.05:
                    doc = dict(prioritized_documents[idx])  # Create a new dict
                    doc['similarity_score'] = similarity_score
                    doc['match_type'] = 'tfidf'
                    tfidf_matches.append(doc)
                    tfidf_matches.append(doc)
            
            # Combine and deduplicate results
            all_matches = {}
            
            # Add exact matches first (higher priority)
            for doc in exact_matches:
                file_name = doc['file_name']
                all_matches[file_name] = doc
            
            # Add TF-IDF matches if not already included
            for doc in tfidf_matches:
                file_name = doc['file_name']
                if file_name not in all_matches:
                    all_matches[file_name] = doc
                elif all_matches[file_name]['similarity_score'] < doc['similarity_score']:
                    # Keep the higher score but mark as hybrid
                    all_matches[file_name]['similarity_score'] = max(
                        all_matches[file_name]['similarity_score'], 
                        doc['similarity_score']
                    )
                    all_matches[file_name]['match_type'] = 'hybrid'
            
            # Sort by similarity score and return top results
            relevant_docs = list(all_matches.values())
            relevant_docs.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            return relevant_docs[:top_k]
            
        except Exception as e:
            print(f"Error during retrieval: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
            date_patterns.extend(re.findall(r'\b\d{4}[/-]\d{1,2}\b', query_lower))
            
            # Add the full query for exact matching
            search_terms = [query_lower] + date_patterns
            
            for i, doc in enumerate(self.documents):
                content_lower = doc.get('content', '').lower()
                max_score = 0
                match_found = False
                
                for term in search_terms:
                    if term in content_lower:
                        match_found = True
                        # Score based on term length and specificity
                        score = min(1.0, len(term) / 20.0 + 0.5)
                        max_score = max(max_score, score)
                
                if match_found:
                    doc_copy = dict(doc)
                    doc_copy['similarity_score'] = max_score
                    doc_copy['match_type'] = 'exact'
                    exact_matches.append(doc_copy)
            
            # Vectorize the query for TF-IDF search
            query_vector = self.vectorizer.transform([query])
            
            # Calculate similarity scores
            similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()
            
            # Get top-k most similar documents (limit to available documents)
            num_docs = len(self.documents)
            actual_k = min(top_k, num_docs)
            top_indices = np.argsort(similarities)[::-1][:actual_k]
            
            # Get TF-IDF matches
            tfidf_matches = []
            for i in range(len(top_indices)):
                idx = int(top_indices[i])  # Convert to Python int
                similarity_score = float(similarities[idx])  # Convert to Python float
                
                # Apply minimum similarity threshold (lowered for better recall)
                if similarity_score > 0.05:
                    doc = dict(self.documents[idx])  # Create a new dict
                    doc['similarity_score'] = similarity_score
                    doc['match_type'] = 'tfidf'
                    tfidf_matches.append(doc)
            
            # Combine and deduplicate results
            all_matches = {}
            
            # Add exact matches first (higher priority)
            for doc in exact_matches:
                file_name = doc['file_name']
                all_matches[file_name] = doc
            
            # Add TF-IDF matches if not already included
            for doc in tfidf_matches:
                file_name = doc['file_name']
                if file_name not in all_matches:
                    all_matches[file_name] = doc
                elif all_matches[file_name]['similarity_score'] < doc['similarity_score']:
                    # Keep the higher score but mark as hybrid
                    all_matches[file_name]['similarity_score'] = max(
                        all_matches[file_name]['similarity_score'], 
                        doc['similarity_score']
                    )
                    all_matches[file_name]['match_type'] = 'hybrid'
            
            # Sort by similarity score and return top results
            relevant_docs = list(all_matches.values())
            relevant_docs.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            return relevant_docs[:top_k]
            
        except Exception as e:
            print(f"Error during retrieval: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def search_documents(self, query: str, top_k: int = 3) -> str:
        """
        Search documents and return formatted context string.
        
        Args:
            query (str): The search query
            top_k (int): Number of top documents to retrieve
            
        Returns:
            str: Formatted context string
        """
        relevant_docs = self.retrieve_relevant_chunks(query, top_k)
        
        if not relevant_docs:
            return "No relevant documents found."
        
        context = ""
        for i, doc in enumerate(relevant_docs):
            score = doc.get('similarity_score', 0.0)
            file_name = doc.get('file_name', 'Unknown')
            content = doc.get('content', '')
            
            context += f"Relevant Document {i+1} - {file_name} (Score: {score:.3f}):\n"
            
            # Limit content length
            if len(content) > 1000:
                context += content[:1000] + "...\n\n"
            else:
                context += content + "\n\n"
        
        return context


if __name__ == "__main__":
    # Test the retriever with sample documents
    sample_docs = [
        {"file_name": "doc1.txt", "content": "Python is a programming language used for web development."},
        {"file_name": "doc2.txt", "content": "Machine learning is a subset of artificial intelligence."},
        {"file_name": "doc3.txt", "content": "Web development involves creating websites and web applications."}
    ]
    
    retriever = SimpleRetriever(sample_docs)
    
    query = "What is Python used for?"
    relevant = retriever.retrieve_relevant_chunks(query)
    
    print(f"Query: {query}")
    print(f"Found {len(relevant)} relevant documents:")
    for doc in relevant:
        print(f"- {doc['file_name']} (Score: {doc['similarity_score']:.3f})")
